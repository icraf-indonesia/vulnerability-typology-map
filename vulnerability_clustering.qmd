---
title: "vulnerability_clusters"
format: 
  html:
    code-overflow: wrap
    code-fold: true
    toc: true
    number-sections: true
editor: visual
---


# Introduction

The objective here is to present a typology of climate vulnerability, identifying factors and indicators that determine a village's susceptibility to climate change.

## Loading the data
**Reading the Data:** We have an Excel file with lots of information. Each row in the file gives us details about a village, and the columns tell us about different factors that might make the village more or less vulnerable to climate change. There's also a special sheet in the Excel file that explains what all these factors mean, their unique IDs, and how they're measured.

### The main dataset `raw_df`
```{r}
#| warning: false
library(readxl)
library(dplyr)
library(corrplot)
library(caret)

df_raw <- 
  read_xlsx("data/typology_podes2019_sulsel_var_HA_rev.xlsx", 
            sheet = 2) |> 
  slice(-1) |>  # remove unnecessary row
  select(-c(id, sumber, kdkab, kdkec, kddesa, kdprov, periode, iddummy_, idkec_dummy)) |> # remove unnecessary columns
  mutate(across(-c(nmprov, nmkab, nmkec, nmdesa), as.numeric)) |>  # convert predictors to numeric
  select(-c(luas_desa, # duplicate to T01
            T09, T08, T07 # categorical values
            ))

glimpse(df_raw)
```
Now we have a big table called `df_raw`. This table contains information like the name of the province, district, sub-district, and village. It also includes data about the village's area, population density, and lots of other numerical factors. 


### The lookup table`df_lookup`
```{r}
#| warning: false
df_lookup <- 
  read_xlsx("data/typology_podes2019_sulsel_var_HA_rev.xlsx", 
            sheet = 1) |> select(ID, Code, Tematik, Unit, Desc, Sulsel)

glimpse(df_lookup)
```
The `df_lookup` table serves as a legend for understanding specific codes used throughout our dataset. Each row represents a unique code and provides detailed information about various aspects of land use and environmental metrics at the village level.


# Preparing the data

Before diving into the PCA, let's ensure that the data meets the necessary requirements.

## Exclude identifiers

```{r}
#| warning: false
df_pre_pca <- df_raw |> select(-nmprov, -nmkab, -nmkec, -nmdesa, -iddesa, -idkec,)
glimpse(df_pre_pca)
```


## Exclude constant or near constant columns
In Principal Component Analysis (PCA), you want to reduce the dimensionality of your dataset to uncover patterns and make visualization easier. However, if some variables (columns) have constant or near-constant values across the observations, they can't contribute to explaining the variability in your data. Such columns should be removed before performing PCA. Here's some code that identify these columns:

```{r}
# Set the threshold
threshold <- 0.025

# Find the standard deviation for each numeric column
std_devs <- apply(df_pre_pca, MARGIN =  2, FUN = sd) # Excluding the first 5 columns that are non-numeric

# Identify the columns with standard deviation below the threshold
near_constant_columns <- names(std_devs)[std_devs < threshold]

df_lookup |> filter(Code %in% near_constant_columns)
```

```{r}
# Exclude those columns from the dataset
df_pre_pca <- df_pre_pca[, !(names(df_pre_pca) %in% near_constant_columns)]

names(df_pre_pca)
```


<!-- ## log10 transformation -->
<!-- ```{r} -->
<!-- # Apply the log10 transformation, adding 1 to handle zeros -->
<!-- df_pre_pca <- df_pre_pca |> mutate(across(-T06, ~log10(.x))) #ignore T06 as it has been scaled -->

<!-- ``` -->


## Check for missing values
```{r}
# Check for missing values
missing_data <- sapply(df_pre_pca, function(x) sum(is.na(x)))
print(missing_data)
```
No missing data is found. Handle missing values if any, possibly through imputation

## Scale the data

Scaling is vital for PCA since it's sensitive to the variances of the variables. Standardizing ensures that each variable contributes equally to the principal components.
```{r}
# Scale the log-transformed data
df_pre_pca <- df_pre_pca %>%
  mutate(across(-T06, scale)) #ignore T06 as it has been scaled
```

## Identify and remove multi-collinear variables

Identifying and removing multicollinearity is an essential step before performing PCA, as PCA assumes that the features are linearly independent. Compute the correlation matrix and identify highly correlated variables:
```{r}
cor_matrix <- cor(df_pre_pca)
corrplot(cor_matrix, method="circle")
```
**Systematically Remove Multicollinear Variables**: decide on a correlation threshold that indicates multicollinearity (e.g., 0.8) and then manually or programmatically remove one variable from each pair or group that exceeds this threshold. The removed variables are:

```{r}
# Set a threshold value for correlation
threshold <- 0.8

# Find the indices of the variables that are highly correlated according to the specified threshold in the correlation matrix 'cor_matrix.'
# This will be used to remove the highly correlated variables from 'df_pre_pca.'
highly_correlated <- findCorrelation(cor_matrix, cutoff = threshold, names = FALSE)

# If you also need the names of the highly correlated variables, you can extract them using the indices.
highly_correlated_names <- colnames(cor_matrix)[highly_correlated]

# Optional: If you need to filter 'df_lookup' by the highly correlated variable names
df_lookup_filtered <- df_lookup |> filter(Code %in% highly_correlated_names)

# Remove the columns from 'df_pre_pca' that correspond to the indices of the highly correlated variables.
df_pre_pca <- df_pre_pca[,-highly_correlated]

print(df_lookup_filtered)
```

# Exploratory data analysis (EDA)

Visualize the data and examine any patterns, distributions, or outliers.

## Boxplot

```{r}
# Histograms
boxplot(df_pre_pca)

```

# PCA Analysis
```{r}
# Perform PCA
pca_result <- prcomp(df_pre_pca)

# Print the summary of the PCA result
summary(pca_result)

# Visualize the PCA result using a scree plot to see the variance explained by each principal component
plot(pca_result, type = "l")

```

```{r}
biplot(pca_result)

```


# To do next

Reducing the number of variables included in the PCA analysis can be done based on the significance of the loadings for the top principal components. By focusing on these significant variables, we can concentrate on the ones that contribute the most to the variance in your data.

