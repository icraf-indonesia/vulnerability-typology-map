---
title: "vulnerability_clusters"
format: 
  html:
    code-overflow: wrap
    code-fold: true
    toc: true
    number-sections: true
editor: visual
editor_options: 
  chunk_output_type: console
---

# Introduction

The objective here is to present a typology of climate vulnerability, identifying factors and indicators that determine a village's susceptibility to climate change.

## Pipeline

A flowchart includes steps in performing PCA.

## Loading the data

**Reading the Data:** We have an Excel file with lots of information. Each row in the file gives us details about a village, and the columns tell us about different factors that might make the village more or less vulnerable to climate change. There's also a special sheet in the Excel file that explains what all these factors mean, their unique IDs, and how they're measured.

### The main dataset `raw_df`

```{r}
#| warning: false
library(readxl)
library(dplyr)
library(corrplot)
library(caret)
#library(gtExtras)
#library(naniar)

df_raw <-
  read_xlsx("data/Analisis_Tipologi_NTT.xlsx",
            sheet = "summary_all_copy") |> 
  janitor::clean_names() #|> 
  #mutate(across(-c(idkec_dum, sumber, nmprov, nmkab, nmkec), as.numeric))

head(df_raw)
```

Now we have a big table called `df_raw`. This table contains information like the name of the province, district, sub-district, and village. It also includes data about the village's area, population density, and lots of other numerical factors.

```{r}
# Remove unnecessary columns from the original dataframe 'df_raw'
# The 'dplyr::select()' function is used to exclude these columns
df_gt <- df_raw |> 
  dplyr::select(-c(id, nmprov, nmkab, nmkec, idkec))

df_gt_exclude <- df_gt |>  dplyr::select(annual_temperature_c_change, precipitation_change, ndwi_2020, ndmi_2020)

# Add a small constant (0.001) to every value in 'df_gt'
# This is often done to allow for log transformations of data that includes zeros
df_gt_plus <- df_gt + 0.001

# Log-transform and scale the data
# 1. 'as_tibble()' converts the dataframe to a tibble for easier manipulation
# 2. 'mutate_all(.funs = log10)' applies the base-10 logarithm to all columns
# 3. 'scale()' standardizes each column so that it has mean=0 and sd=1
unscaled_df <- df_gt_plus |> 
  dplyr::select(-annual_temperature_c_change,-precipitation_change, -ndwi_2020, -ndmi_2020) |> as_tibble() |> mutate_all(.funs = log10) 

# Add back the columns that were removed earlier to create a complete, scaled dataframe
# 'bind_cols()' binds the selected columns from 'df_raw' and the scaled columns from 'scaled_df'
scaled_df <- df_raw |>
  dplyr::select(
    c(
      annual_temperature_c_change,
      precipitation_change,
      ndwi_2020,
      ndmi_2020
    )
  ) |>
  bind_cols(unscaled_df)|> scale() 

scaled_df_complete <- df_raw |> dplyr::select(c(id, nmprov, nmkab, nmkec, idkec)) |> bind_cols(scaled_df)

# Remove the column 'korban_jiwa_kebakaran_hutan_dan_lahan_2018_2019' from the complete, scaled dataframe
scaled_df_complete_temp <- scaled_df_complete |>
  dplyr::select(-korban_jiwa_kebakaran_hutan_dan_lahan_2018_2019, -korban_jiwa_banjir_bandang_2018_2019)

```

# Preparing the data

Before diving into the PCA, let's ensure that the data meets the necessary requirements.

## Check for missing values

```{r}
# Check for missing values
missing_data <- sapply(scaled_df_complete_temp, function(x)
  sum(is.na(x)))
print(missing_data)

scaled_df_complete_temp %>%
  filter_all(any_vars(is.na(.)))

#remove missing data
# scaled_df_complete <- scaled_df_complete[complete.cases(scaled_df_complete), ]
```

## Exclude identifiers

```{r}
#| warning: false
df_pre_pca <- scaled_df_complete_temp |> 
  dplyr::select(
    -c(
      id,
      nmprov,
      nmkab,
      nmkec,
      idkec))

glimpse(df_pre_pca)
```

<!-- ## Exclude constant or near constant columns -->

<!-- In Principal Component Analysis (PCA), you want to reduce the dimensionality of your dataset to uncover patterns and make visualization easier. However, if some variables (columns) have constant or near-constant values across the observations, they can't contribute to explaining the variability in your data. Such columns should be removed before performing PCA. Here's some code that identify these columns: -->

<!-- ```{r} -->

<!-- # Set the threshold -->

<!-- threshold <- 0.025 -->

<!-- # Find the standard deviation for each numeric column -->

<!-- std_devs <- apply(df_pre_pca, MARGIN =  2, FUN = sd, na.rm=TRUE) # Excluding the first 5 columns that are non-numeric -->

<!-- # Identify the columns with standard deviation below the threshold -->

<!-- near_constant_columns <- names(std_devs)[std_devs < threshold] -->

<!-- ``` -->

<!-- ```{r} -->

<!-- # Exclude those columns from the dataset -->

<!-- df_pre_pca <- df_pre_pca[, !(names(df_pre_pca) %in% near_constant_columns)] -->

<!-- names(df_pre_pca) -->

<!-- ``` -->

<!-- ## log10 transformation -->

<!-- ```{r} -->

<!-- # Apply the log10 transformation, adding 1 to handle zeros -->

<!-- df_pre_pca <- df_pre_pca |> mutate(across(-T06, ~log10(.x))) #ignore T06 as it has been scaled -->

<!-- ``` -->

No missing data is found. Handle missing values if any, possibly through imputation

## Scale the data

## Identify and remove multi-collinear variables

Identifying and removing multicollinearity is an essential step before performing PCA, as PCA assumes that the features are linearly independent. Compute the correlation matrix and identify highly correlated variables:

```{r}
df_pre_pca_abb <- df_pre_pca
# Abbreviate column names using R's built-in abbreviate function
abbrev_colnames <- abbreviate(colnames(df_pre_pca_abb))

# Assign the abbreviated names back to the dataframe
colnames(df_pre_pca_abb) <- abbrev_colnames

# Recalculate and plot the correlation matrix
cor_matrix <- cor(df_pre_pca_abb)
corrplot(cor_matrix, method="circle", tl.cex = 0.5, type = "lower")
```

**Systematically Remove Multicollinear Variables**: decide on a correlation threshold that indicates multicollinearity (e.g., 0.8) and then manually or programmatically remove one variable from each pair or group that exceeds this threshold. The removed variables are:

```{r}
# Set a threshold value for correlation
threshold <- 0.8

# Find the indices of the variables that are highly correlated according to the specified threshold in the correlation matrix 'cor_matrix.'
# This will be used to remove the highly correlated variables from 'df_pre_pca.'
highly_correlated <- findCorrelation(cor(df_pre_pca), cutoff = threshold, names = TRUE, verbose = TRUE)

# # If you also need the names of the highly correlated variables, you can extract them using the indices.
# highly_correlated_names <- colnames(cor(df_pre_pca))[highly_correlated]


print(highly_correlated)
# Remove the columns from 'df_pre_pca' that correspond to the indices of the highly correlated variables.
# df_pre_pca <- df_pre_pca |> dplyr::select(-highly_correlated_names[[1]])

```

# Exploratory data analysis (EDA)

Visualize the data and examine any patterns, distributions, or outliers.

```{r}
#| warning: false

# Pivot the 'scaled_df_complete_temp' dataframe to long format
# Exclude certain columns from being pivoted and specify the new column names for 'variable' and 'value'
df_long <- scaled_df_complete_temp |> 
  #dplyr::select(1:9) |> bind_cols(df_pre_pca) |> 
  tidyr::pivot_longer(
    cols = -c(id, nmprov, nmkab, nmkec, idkec),
    names_to = "variable",
    values_to = "value"
  )

# Select only the 'variable' and 'value' columns from the long-form data
# Remove unwanted columns and group by 'variable' to prepare for summarization
gt_tab <- df_long |>
  dplyr::select(-c(idkec_dum, sumber, kdprov, nmprov, nmkab, nmkec, periode, kdkab, kdkec)) |>
  group_by(variable) 

# Calculate summary statistics for each group (each 'variable')
# Also, keep the original 'value' data in a list column called 'value_data'
# Use '.groups = "drop"' to return a regular dataframe rather than a grouped one
gt_stat <- gt_tab |> 
  summarise(
    min = min(value, na.rm = TRUE),
    max = max(value, na.rm = TRUE),
    median = median(value, na.rm = TRUE),
    #mean = mean(value, na.rm = TRUE),
    #sd = sd(value, na.rm = TRUE),
    value_data = list(value),
    .groups = "drop"
  )

# Create a GT table with a density plot column
# The density plot is generated from the 'value_data' list column
# Customize the appearance of the density plot and format the number columns
gt_stat |>
  gt::gt() |> 
  gtExtras::gt_plt_dist(
    value_data,
    type = "density",
    line_color = "blue",
    fill_color = "red"
  ) |> gt::fmt_number(columns = min:median, decimals = 1)

```

```{r}
## Boxplot
# Histograms
boxplot(df_pre_pca)

```

## Density plots

```{r}

# Start the PNG device
png("output/density_plots.png", width = 2000, height = 2800)

# Set up the plot layout
par(mfrow = c(7, 6))

# Define the font size
font_size <- 4

# Loop through the columns of the data frame
for (i in 1:ncol(df_pre_pca)) {
  # Get the current variable
  variable <- df_pre_pca[, i]
  
  # Create a density plot for the variable with larger font size
  plot(density(variable[[1]]), main = "", xlab = "", ylab = "", cex.axis = font_size, cex.lab = font_size)
  
  # Add the title using the column name with larger font size
  title(main = colnames(df_pre_pca)[i], cex.main = font_size)
}

# Close the PNG device
dev.off()
```

# PCA Analysis

```{r}
# Perform PCA
pca_result <- prcomp(df_pre_pca)
```

# Future Steps

## Reducing Variables in PCA Analysis

By selecting variables based on the significance of loadings for the top principal components, we can focus on the ones that contribute most to the data's variance. This reduction simplifies the analysis and emphasizes the most influential factors.

## Understanding the Principal Components (PCs)

### The Biplot

Biplots graphically represent the relationship between observations and variables, providing insight into the PCs.

```{r}
biplot(pca_result)
```

### The Scree Plot

A scree plot visualizes the proportion of variance explained by each PC. The first few PCs are the main contributors, while the rest have minimal influence.

```{r}
# Visualize the PCA result using a scree plot to see the variance explained by each principal component
plot(pca_result, type = "l")

```

### A Look at Loadings

Loadings in PCA reveal how original variables influence each Principal Component. High absolute values indicate strong influence, while the sign indicates the direction. Understanding these can uncover underlying themes, such as climate factors.

```{r}
# Extract the loadings for the first three principal components
loadings <- pca_result$rotation[, 1:3]

# Set up the plotting area with 1 row and 3 columns
par(mfrow = c(1, 3), mar = c(5, 4, 4, 2))

# Extract the loadings for the first three principal components
loadings <- pca_result$rotation[, 1:3]

# Plot the loadings for the first principal component
barplot(loadings[, 1], main = "Loadings for PC1", names.arg = rownames(loadings), las = 2, cex.names = 0.7)

# Plot the loadings for the second principal component
barplot(loadings[, 2], main = "Loadings for PC2", names.arg = rownames(loadings), las = 2, cex.names = 0.7)

# Plot the loadings for the third principal component
barplot(loadings[, 3], main = "Loadings for PC3", names.arg = rownames(loadings), las = 2, cex.names = 0.7)

```

#### The importance of the factors for each principal component

```{r}
# Extract the loadings for the first three principal components
loadings <- pca_result$rotation[, 1:3]

# Create a data frame with the loadings
loadings_df <- as.data.frame(loadings)
loadings_df$Variable <- rownames(loadings)

# Create a function to sort the factors by influence for a given principal component
sort_factors <- function(pc) {
  sorted_factors <- loadings_df[order(-abs(loadings_df[, pc])), c("Variable", pc)]
  colnames(sorted_factors) <- c("Variable", "Loading")
  return(sorted_factors)
}

# Sort the factors for each of the first three principal components
sorted_factors_PC1 <- sort_factors("PC1")
sorted_factors_PC2 <- sort_factors("PC2")
sorted_factors_PC3 <- sort_factors("PC3")

# Print the sorted factors for each principal component
print(sorted_factors_PC1)
print(sorted_factors_PC2)
print(sorted_factors_PC3)

readr::write_csv(sorted_factors_PC1,"output/PC1_sorted.csv")
readr::write_csv(sorted_factors_PC2,"output/PC2_sorted.csv")
readr::write_csv(sorted_factors_PC3,"output/PC3_sorted.csv")
```

Interpret the tables above to understand the importance of the factors for each principal component. The magnitude of the loading (regardless of sign) indicates the strength of the contribution, and the sign of the loading indicates whether the contribution is positive or negative.

#### Least influential factors

To identify the factors that are consistently least influential across the first three principal components, we can look at the absolute values of the loadings. Factors with small absolute loadings are less influential.

```{r}
# Convert the loadings to a data frame
loadings_df <- as.data.frame(loadings)

# Calculate the absolute loadings
abs_loadings <- abs(loadings_df)

# Calculate the average absolute loading for each factor
avg_influence <- rowMeans(abs_loadings)

# Create a data frame with the average influence
influence_df <- data.frame(Variable = rownames(abs_loadings), AverageInfluence = avg_influence)

# Sort the factors by average influence
sorted_influence <- influence_df |> arrange(-AverageInfluence) 

# Print the sorted factors
print(sorted_influence)

```

The result will be a table that shows the factors sorted by their average absolute influence across the first three principal components. The factors at the top of the table are the least influential.

### Insights from Domain Experts

Knowledge of the specific field aids in interpreting PCs. Principal Components can be complex and may resist straightforward interpretation.

## Classifying Data with K-Means Clustering and Principal Components

### Step 1: Selecting the Principal Components

Start by using the top three Principal Components from PCA, which capture the core variances and correlations in your data.

```{r}
# Extract the first three principal components
first_three_pcs <- pca_result$x[, 1:12]
```

### Step 2: Determine the Number of Clusters

Select the optimal number of clusters (k) using methods like the Elbow Method or Silhouette Analysis.

#### Elbow Method

The Elbow Method involves running k-means clustering for a range of $k$ values and plotting the total within-cluster sum of squares. The "elbow" of the plot represents an optimal value for $k$ (a balance between precision and computational cost).

```{r}
# Compute total within-cluster sum of squares for different k values
wss <- sapply(1:10, function(k) {
  kmeans(first_three_pcs, centers = k)$tot.withinss
})

# Plot the total within-cluster sum of squares
plot(1:10, wss, type = "b", xlab = "Number of Clusters", ylab = "Total Within-Cluster Sum of Squares",
     main = "Elbow Method")
```

based on the numbers, you might consider the point where the decrease starts to slow down, which could be around $k$= 4 or $k$=5.

#### Silhouette Analysis

Silhouette Analysis measures how similar an object is to its own cluster compared to other clusters. The silhouette score ranges from -1 to 1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.

```{r}
library(cluster)

# Compute silhouette scores for different k values
silhouette_scores <- sapply(2:10, function(k) {
  cluster_result <- kmeans(first_three_pcs, centers = k)
  silhouette_avg <- mean(silhouette(cluster_result$cluster, dist(first_three_pcs))[, "sil_width"])
  silhouette_avg
})

# Plot the silhouette scores
plot(2:10, silhouette_scores, type = "b", xlab = "Number of Clusters", ylab = "Average Silhouette Width",
     main = "Silhouette Analysis")

```

The maximum silhouette score is 0.3428859, which corresponds to 5 clusters (since the first value represents $k$=2)

### Step 3: Applying K-Means Algorithm

Utilize the k-means algorithm to divide the data into k clusters, focusing on the first three PCs. In R, this can be done with:

```{r}
#| eval: true
# Choose the number of clusters
k <- 5

# Perform k-means clustering
kmeans_result <- kmeans(first_three_pcs, centers = k)
```

### Step 4: Interpret the Clusters

Investigate each cluster to understand the common traits within them. Interpretation requires a blend of data analysis and domain-specific knowledge.

```{r}
library(ggplot2)

# Create a data frame for plotting
plot_data <- data.frame(first_three_pcs, cluster = as.factor(kmeans_result$cluster))

# Plot the first two principal components and color by cluster
ggplot(plot_data, aes(x = PC2, y = PC3, color = cluster)) +
  geom_point() +
  labs(title = "K-means Clustering on First Two Principal Components") +
  theme_minimal()

```

An interactive 3D scatter plot below shows the first three principal components, colored by cluster. We can rotate the plot to view it from different angles, and you can hover over the points to see additional information.

```{r}

# Load the plotly package
library(plotly)


# Create a data frame for plotting
plot_data <- data.frame(first_three_pcs, cluster = as.factor(kmeans_result$cluster))

# Create the 3D scatter plot
plot_3d <- plot_ly(data = plot_data, x = ~PC1, y = ~PC2, z = ~PC3, color = ~cluster) 
# Show the plot
plot_3d


```

# Exploratory data analysis

## Grids of box plots for each parameters

```{r}
pca_eda <- tibble(df_pre_pca, cluster = as.factor(kmeans_result$cluster))

library(ggplot2)
library(gridExtra)

# Convert the data frame from a wide to a long format  
pca_eda_long <- pca_eda|> 
  tidyr::gather(key = "variable", value = "value", -cluster)

#Create a list of ggplot objects for each variable
plots <- lapply(unique(pca_eda_long$variable), function(var) {
  ggplot(pca_eda_long[pca_eda_long$variable == var,], aes(x = cluster, y = value)) +
    geom_boxplot() +
    labs(title = var, x = "Cluster", y = "Value") +
    theme_minimal()
})
grid_plot <- do.call(gridExtra::grid.arrange, c(plots, ncol = 5))

ggsave("output/boxplots.png", grid_plot, width = 20, height = 15)

```

## Visualise the clusters into a map

# Read a map then attach

```{r}
library(terra)
library(sf)
# Read the shapefile
desa <- st_read("data/INDO_DESA_2019/INDO_DESA_2019.shp")

# Filter based on the 'kdprov' attribute
desa_ntt <- desa |> filter(nmprov %in% "NUSA TENGGARA TIMUR")
desa_ntt <- desa_ntt |> dplyr::select(-c("kddesa", "iddesa"))
desa_ntt_id_kec <- desa_ntt |> select(nmkab,nmkec,kdprov,kdkab,kdkec) |> 
  mutate(idkec = paste0(kdprov,kdkab,kdkec) ) |>
  mutate(idkec = as.numeric(idkec))
rm(desa)

# add the cluster attribute to the desa_sulsel object
cluster_data<- scaled_df_complete_temp |> 
  select(-c(id, nmkab, nmkec ,nmprov)) |> 
  #dplyr::select(1:9) |>
  bind_cols(tibble (cluster = kmeans_result$cluster))

clusters_ntt <- desa_ntt_id_kec |> left_join(cluster_data, by = "idkec")


clusters_ntt_short <- clusters_ntt |> dplyr::select(idkec, nmkec, cluster)

# Write the sf object to a shapefile
st_write(clusters_ntt_short, "output/tipologi_5_kelas_ntt_short_12_PC.shp", append = TRUE)

# # Plot the SpatVector object, using the 'cluster' attribute for the fill color
# plot(desa_sulsel, "cluster", col=c("#E69F00", "#CC79A7", "#009E73", "#F0E442", "#0072B2"), lwd=0.1) # Assuming we have 5 clusters

library(leaflet)
# Create a color palette for the 'cluster' variable
pal <- colorFactor(palette = "Set1", domain = clusters_ntt_short$cluster)

# Create the leaflet map
leaflet(clusters_ntt_short) %>%
  addProviderTiles(providers$OpenStreetMap) %>%
  addPolygons(
    fillColor = ~pal(cluster),
    weight = 0.5,
    opacity = 1,
    color = "white",
    dashArray = "3",
    fillOpacity = 0.7,
    highlight = highlightOptions(
      weight = 5,
      color = "#666",
      dashArray = "",
      fillOpacity = 0.7,
      bringToFront = TRUE
    ),
    label = ~paste0("<strong>Kabupaten: </strong>", nmkab,
                    "<br><strong>Kecamatan: </strong>", nmkec,
                    "<br><strong>Cluster: </strong>", cluster)
  ) %>%
  addLegend(pal = pal, values = ~cluster, title = "Cluster")


```

# Caveats

-   **Lack of Data Understanding**: The analysis was conducted without in-depth knowledge of the data, so the results should be interpreted cautiously.

-   **Skewed Distribution**: Many variables have a right-skewed distribution, which may violate the linearity assumption if not properly transformed.

-   **No one-size-fits-all**: Data pre-processing should be tailored to the specific dataset and analysis objectives. This includes careful consideration of data transformation and variable selection, based on both statistical properties and subject-matter knowledge.
