---
title: "Assessing and Characterizing Climate Vulnerabilities in Agriculture-Based Livelihoods: South Sulawesi"
format: 
  html:
    code-overflow: wrap
    code-fold: true
    toc: true
    number-sections: true
editor: visual
project:
  type: website
  output-dir: docs
editor_options: 
  chunk_output_type: console
---

# Introduction

::: panel-tabset
## Abstrak

Penghidupan berbasis pertanian kini makin rentan terhadap perubahan iklim, tetapi informasi mengenai potensi resiko dan kebutuhan adaptasi mereka masih sangat terbatas. Bahan ajar ini disusun untuk mengisi kekosongan ini dengan mengevaluasi berbagai jenis kerentanan yang mempengaruhi mata pencaharian berbasis pertanian di tingkat provinsi. Kami melakukan penilaian kerentanan untuk mengidentifikasi risiko utama serta akar penyebabnya, dan potensi adaptasi, dengan fokus pada peningkatan taraf hidup, keberlanjutan produksi komoditas-komoditas kunci, dan pengelolaan lahan secara menyeluruh.

Mengingat keanekaragaman kondisi di masing-masing provinsi, kami memfokuskan perhatian pada kecamatan-kecamatan dengan fitur biofisik dan sosial-ekonomi yang mirip. Ini membantu kami mempermudah tugas dalam mengidentifikasi risiko yang identik antar kecamatan. Kami mendefinisikan area-area homogen ini, atau 'tipologi,' dengan menggunakan pengelompokan K-means. Pengelompokan ini didasarkan pada komposit dari indikator biofisik dan sosial-ekonomi. Untuk mempermudah proses pengelompokan, kami menggunakan analisis PCA untuk menyederhanakan dimensi data.

## Abstract

Smallholder farmers are increasingly vulnerable to climate change, yet little is known about their specific risks and adaptation requirements. Our study aims to address this gap by examining the different vulnerabilities affecting agriculture-based livelihoods at the provincial level. We conducted climate vulnerability assessments to identify key risks and their root causes, as well as possible mitigation measures, with a focus on sustaining livelihoods, key commodities production, and overall land management.

Considering the diverse conditions of the study area, we narrowed our focus to locations with similar biop hysical and socio-economic features. This helped us streamline the task of spotting common risks across communities. We defined these homogeneous areas, or 'typologies,' using K-means clustering. The clustering was based on a composite of biophysical and socio-economic indicators. To make the clustering process more precise, we applied PCA analyses to simplify the data dimensions.

## Objective

We aimed to identify homogeneous areas, or 'typologies', with similar biophysical and socio-economic features across South Sulawesi Province, using K-means clustering on PCA-simplified data to infer common risks across communities.
:::

# Study area & Data

::: panel-tabset
## Study area

The South Sulawesi province, located in the southern peninsula of Sulawesi, Indonesia, spans an area of 46,717 square kilometres. The province is characterised by a north-south chain of mountains, topped by volcanic cones and bisected midway by the Tempe Lake valley. The climate is characterised by high rainfall throughout the year, typical of tropical rainforest climates.

South Sulawesi is home to a population of approximately 8.851 million people as of 2019. The province is characterised by a moderate level of income inequality, as reflected in the Gini ratios for its districts, which ranged from around 0.32 to 0.40 in 2022. The province scored 7.22 on the Human Development Index in 2022.

The Gross Domestic Product (GDP) of South Sulawesi was approximately 605.145 billion Indonesian Rupiah in 2022, which is equivalent to around 4,075 million US dollars. This places South Sulawesi as the 9th largest economy among the provinces in Indonesia.

Agriculture plays a significant role in the province's economy, with an estimated one million farmers operating in South Sulawesi, according to the 2018 Survei Pertanian Antar Sensus (SUTAS) conducted by the Badan Pusat Statistik (BPS). Major agricultural products include rice, corn (maize), copra (dried coconut meat), coffee, spices, vegetable oil, sugarcane, soybeans, and sweet potatoes. The forests of the region yield valuable resources such as teak and rattan, and deep-sea fishing also contributes to the local economy.

## Data Collection

Understanding the vulnerability of smallholder farmers to climate change involves examining a variety of biophysical and socio-economic factors. These factors, represented as grid layers or proxies, are chosen based on literature reviews, field observations, and expert insights. However, not all potential variables are readily available or of sufficient quality for analysis.

These factors are organised into five categories, known as livelihood capitals: natural, physical, human, social, and financial. Each represents a unique aspect that can influence a farmer's vulnerability to climate change.

-   Natural capital includes essential natural resources like forests, rivers, and arable land, as well as climate factors such as temperature and precipitation.

-   Physical capital refers to infrastructure and production resources, including proximity to plantations, roads, and processing facilities.

-   Human capital encompasses the population's knowledge, skills, and health, reflected in variables like village population and unemployment rate.

-   Social capital represents the social networks that provide support and resources to farmers, indicated by factors like the percentage of smallholder agricultural areas in a village.

-   Financial capital involves financial resources, income diversity, and access to credit, represented by variables such as farm size and crop yield.

## Vulnerability indicators with descriptions and data sources

```{r Table 1}
#| echo: false
#| warning: false
library(readxl)
library(corrplot)
library(caret)
library(gtExtras)
library(naniar)
library(knitr)
library(tidyr)
library(leaflet)
library(terra)
library(sf)
library(dplyr)
library(readr)
library(kableExtra)

read_csv("data/vulnerability_indicators.csv") |>
  kbl() |> 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), fixed_thead = T,  full_width = F) |> 
    column_spec(2, width = "30em") |> 
    scroll_box(width = "1000px", height = "800px")
```
:::

## Pipeline

A flowchart includes steps in performing PCA. ![Steps in performing PCA](data/workflow.png)

## Loading the data

**Reading the Data:** We have an Excel file with lots of information. Each row in the file gives us details about a village, and the columns tell us about different factors that might make the village more or less vulnerable to climate change. There's also a special sheet in the Excel file that explains what all these factors mean, their unique IDs, and how they're measured.

### The main dataset `raw_df`

```{r Read raw data}
#| warning: false


df_raw <-
  read_xlsx("data/analisa_tipologi_sulsel.xlsx",
            sheet = "fin_copas") |> 
  janitor::clean_names() |> dplyr::select(-c(idkec_2, idkec_3)) |> 
  mutate(across(-c(idkec_dum, sumber, nmprov, nmkab, nmkec), as.numeric))
```

```{r a subset of columns of the raw data}
#| warning: false
#| message: false
# Select a subset of columns for clarity
df_selected <- df_raw|>
  dplyr::select(1,7,8,10,11, 12) |> head()

# Create the table using gt
df_selected|>
  gt()|>
  tab_header(
    title = "A sample of the Data"
  )|>
  cols_label(
    idkec_dum = "ID",
    nmkab = "District",
    nmkec = "Sub-district",
    distance_to_plantation = "Dist. to Plantation (m)",
    distance_to_road = "Dist. to Road (m)",
    distance_to_commodity_processing_factory = "Dist. to Comm.Proc Factory (m)"
  )|>
  fmt_number(
    columns = c(distance_to_plantation, distance_to_road, distance_to_commodity_processing_factory),
    decimals = 2
  )
```

Now we have a big table called `df_raw`. This table contains information like the name of the province, district, sub-district, and village. It also includes data about the village's area, population density, and lots of other numerical factors. We then standardized the selected features/predictors to unit variance and zero mean and combined them into one layer.

```{r transform and standardise the data}
# Remove unnecessary columns from the original dataframe 'df_raw'
# The 'dplyr::select()' function is used to exclude these columns
df_gt <- df_raw |> 
  dplyr::select(-c(idkec_dum, kdprov, sumber, nmprov, nmkab, nmkec, periode, kdkab, kdkec, prec_change))

df_col_unstandardise <- df_raw |> dplyr::select(prec_change)

# Add a small constant (0.001) to every value in 'df_gt'
# This is often done to allow for log transformations of data that includes zeros
df_gt_plus <- df_gt + 0.001



# Log-transform and scale the data
# 1. 'as_tibble()' converts the dataframe to a tibble for easier manipulation
# 2. 'mutate_all(.funs = log10)' applies the base-10 logarithm to all columns
# 3. 'scale()' standardizes each column so that it has mean=0 and sd=1
scaled_df <-
  df_gt_plus |> as_tibble() |> mutate_all(.funs = log10) |> bind_cols(df_col_unstandardise) |>  scale(center = TRUE, scale = TRUE)

# Add back the columns that were removed earlier to create a complete, scaled dataframe
# 'bind_cols()' binds the selected columns from 'df_raw' and the scaled columns from 'scaled_df'
scaled_df_complete <- df_raw |> 
  dplyr::select(c(idkec_dum, kdprov, sumber, nmprov, nmkab, nmkec, periode, kdkab, kdkec)) |>  
  bind_cols(scaled_df)

# Remove the column 'korban_jiwa_kebakaran_hutan_dan_lahan_2018_2019' from the complete, scaled dataframe
scaled_df_complete_temp <- scaled_df_complete |> 
  dplyr::select(-korban_jiwa_kebakaran_hutan_dan_lahan_2018_2019)

```

# Preparing the data

Before diving into the PCA, let's ensure that the data meets the necessary requirements.

## Check for missing values

```{r Remove incomplete kecamatans}
# Check for missing values
missing_data <- sapply(scaled_df_complete_temp, function(x) sum(is.na(x)))

# print missing data\
scaled_df_complete_temp|> filter_all(any_vars(is.na(.))) |>   dplyr::select("ID Kecamatan" = 1, "Nama Kabupaten" = 5, "Nama Kecamatan " = 6) |> kable(caption = "Kecamatan dengan data tidak lengkap")

#remove missing data
scaled_df_complete_temp <- scaled_df_complete_temp[complete.cases(scaled_df_complete_temp), ]

```

## Exclude identifiers

```{r display the data ready for PCA}
#| warning: false
#| messages: false
df_pre_pca <- scaled_df_complete_temp |> 
  select(-c(idkec_dum, sumber, kdprov, nmprov, nmkab, nmkec, periode, kdkab, kdkec))

df_pre_pca |> kbl(caption = "Input data for a PCA Analysis") |>   kable_paper()|>
  scroll_box(width = "1000px", height = "500px")
```

<!-- ## Exclude constant or near constant columns -->

<!-- In Principal Component Analysis (PCA), you want to reduce the dimensionality of your dataset to uncover patterns and make visualization easier. However, if some variables (columns) have constant or near-constant values across the observations, they can't contribute to explaining the variability in your data. Such columns should be removed before performing PCA. Here's some code that identify these columns: -->

No missing data is found. Handle missing values if any, possibly through imputation

## Identify multi-collinear variables

Compute the correlation matrix and identify highly correlated variables:

```{r display correlation matrix}
#| warning: false
#| messages: false
#df_pre_pca_abb <- df_pre_pca
# Abbreviate column names using R's built-in abbreviate function
#abbrev_colnames <- abbreviate(colnames(df_pre_pca_abb))

# Assign the abbreviated names back to the dataframe
#colnames(df_pre_pca_abb) <- abbrev_colnames

# Recalculate and plot the correlation matrix
cor_matrix <- cor(df_pre_pca) |> round(digits = 2)

# Convert the matrix to a data frame
cor_matrix_df <- as.data.frame(cor_matrix)

# Add row names as a new column
cor_matrix_df$Predictors <- rownames(cor_matrix)

# Move the 'RowNames' column to the first position
cor_matrix_df <- cor_matrix_df|> select(Predictors, everything())



# Create the gt table
cor_matrix_df|>
  gt()|>
  data_color(
    columns = -Predictors,  # Exclude the 'RowNames' column from colorization
    colors = scales::col_numeric(
      palette = c("darkred", "white", "darkblue"),
      domain = c(-1, 1)
    )
  ) |> tab_options(container.overflow.x = TRUE, container.overflow.y = TRUE)


```

```{r display multi-collinear variables}
# Set a threshold value for correlation
threshold <- 0.8

# Find the indices of the variables that are highly correlated according to the specified threshold in the correlation matrix 'cor_matrix.'
# This will be used to remove the highly correlated variables from 'df_pre_pca.'
highly_correlated <- findCorrelation(cor(df_pre_pca), cutoff = threshold, names = FALSE)

# If you also need the names of the highly correlated variables, you can extract them using the indices.
highly_correlated_names <- colnames(cor(df_pre_pca))[highly_correlated]


# # Remove the columns from 'df_pre_pca' that correspond to the indices of the highly correlated variables.
# df_pre_pca <- df_pre_pca |> dplyr::select(-highly_correlated_names[[1]])
# Create a data frame from highly_correlated_names and threshold
highly_correlated_df <- data.frame(
  Predictor = highly_correlated_names
 # Threshold = threshold
)

# Create a gt table
highly_correlated_df|>
  gt()|>
  tab_header(
    title = "Highly Correlated Predictors",
    subtitle = paste("Predictors with correlation above", threshold)
  )|>
  cols_label(
    Predictor = "Predictor Name"
   # Threshold = "Correlation Threshold"
  )

```

# Exploratory data analysis (EDA)

Visualize the data and examine any patterns, distributions, or outliers. In Principal Component Analysis (PCA), you want to reduce the dimensionality of your dataset to uncover patterns and make visualization easier. However, if some variables (columns) have constant or near-constant values across the observations, they can't contribute to explaining the variability in your data. Such columns are advised to be removed before performing PCA.

```{r summary of the predictors}
#| warning: false

# Pivot the 'scaled_df_complete_temp' dataframe to long format
# Exclude certain columns from being pivoted and specify the new column names for 'variable' and 'value'
df_long <- scaled_df_complete_temp |> 
  dplyr::select(1:9) |> bind_cols(df_pre_pca) |> 
  tidyr::pivot_longer(
    cols = -c(idkec_dum, sumber, kdprov, nmprov, nmkab, nmkec, periode, kdkab, kdkec),
    names_to = "variable",
    values_to = "value"
  )

# Select only the 'variable' and 'value' columns from the long-form data
# Remove unwanted columns and group by 'variable' to prepare for summarization
gt_tab <- df_long |>
  dplyr::select(-c(idkec_dum, sumber, kdprov, nmprov, nmkab, nmkec, periode, kdkab, kdkec)) |>
  group_by(variable) 

# Calculate summary statistics for each group (each 'variable')
# Also, keep the original 'value' data in a list column called 'value_data'
# Use '.groups = "drop"' to return a regular dataframe rather than a grouped one
gt_stat <- gt_tab |> 
  summarise(
    min = min(value, na.rm = TRUE),
    max = max(value, na.rm = TRUE),
    median = median(value, na.rm = TRUE),
    #mean = mean(value, na.rm = TRUE),
    #sd = sd(value, na.rm = TRUE),
    value_data = list(value),
    .groups = "drop"
  )

# Create a GT table with a density plot column
# The density plot is generated from the 'value_data' list column
# Customize the appearance of the density plot and format the number columns
gt_stat |>
  gt::gt() |> 
  gtExtras::gt_plt_dist(
    value_data,
    type = "density",
    line_color = "blue",
    fill_color = "red"
  ) |> gt::fmt_number(columns = min:median, decimals = 1) |> 
  cols_label(
    variable = "Predictors",
    min = "Minimum",
    max = "Maximum",
    median = "Median",
    value_data = "Density"
  ) 

```

```{r}
## Boxplot
# Histograms
# boxplot(df_pre_pca)

```

## Density plots

```{r}
#| warning: false
#| include: false

# Start the PNG device
png("output/density_plots.png", width = 2000, height = 2800)

# Set up the plot layout
par(mfrow = c(7, 7))

# Define the font size
font_size <- 4

# Loop through the columns of the data frame
for (i in 1:ncol(df_pre_pca)) {
  # Get the current variable
  variable <- df_pre_pca[, i]
  
  # Create a density plot for the variable with larger font size
  plot(density(variable[[1]]), main = "", xlab = "", ylab = "", cex.axis = font_size, cex.lab = font_size)
  
  # Add the title using the column name with larger font size
  title(main = colnames(df_pre_pca)[i], cex.main = font_size)
}

# Close the PNG device
dev.off()
```

# PCA Analysis

```{r perform pca}
# Perform PCA
pca_result <- prcomp(df_pre_pca)
```

```{r pca summary}
# Assuming pca_result is your prcomp object
pca_summary <- summary(pca_result)$importance |> round(digits = 2)

# Convert to tibble and remove row names
pca_summary_tibble <- tibble::rownames_to_column(as.data.frame(pca_summary), var = "Components")

# Create gt table
pca_summary_tibble|>  
  gt()|>
  tab_header(
    title = "Principal Component Analysis Summary",
    subtitle = "Importance of components") |> 
    opt_align_table_header(align = "left")
```

<!-- ### The Biplot -->

<!-- Biplots graphically represent the relationship between observations and variables, providing insight into the PCs. -->

<!-- ```{r} -->

<!-- biplot(pca_result) -->

<!-- ``` -->

## The Scree Plot

A scree plot visualizes the proportion of variance explained by each PC. The first few PCs are the main contributors, while the rest have minimal influence.

```{r Create a scree plot}
# Visualize the PCA result using a scree plot to see the variance explained by each principal component
plot(pca_result, type = "l", main = "Scree Plot")

```

## A Look at Loadings

Loadings in PCA reveal how original variables influence each Principal Component. High absolute values indicate strong influence, while the sign indicates the direction. Understanding these can uncover underlying themes, such as climate factors.

#### The importance of the factors for each principal component

```{r visualise loadings}
# Extract the loadings for the desired number of principal components
num_pc <- 5
loadings <- pca_result$rotation[, 1:num_pc]

# Function to plot ordered loadings
plot_ordered_loadings <- function(loadings, pc_num) {
  ordered_indices <- order(-abs(loadings[, pc_num]), decreasing = TRUE)
  
  # Determine the colours based on the sign of the values
  bar_colours <- ifelse(loadings[ordered_indices, pc_num] < 0, "red", "blue")
  
  barplot(abs(loadings[ordered_indices, pc_num]), horiz = TRUE,
          main = paste("Loadings for PC", pc_num),
          names.arg = rownames(loadings)[ordered_indices],
          las = 1, cex.names = 0.7, xlim = c(0, 0.5), col = bar_colours)
}

# Set up the plotting area with adjusted margins
par(mar = c(5, 25, 4, 2))

# Loop over each principal component and plot
for (i in 1:num_pc) {
  plot_ordered_loadings(loadings, i)
}


```

To identify the factors that are consistently least influential across the first three principal components, we can look at the absolute values of the loadings. Factors with small absolute loadings are less influential. Knowledge of the specific field aids in interpreting PCs. Principal Components can be complex and may resist straightforward interpretation.

# Classifying Data with K-Means Clustering

### Step 1: Selecting the Principal Components

Start by using the top five Principal Components from PCA, which capture the core variances and correlations in your data.

```{r select the first 5 PCs}
# Extract the first five principal components
selected_components <- pca_result$x[, 1:5]
```

### Step 2: Determine the Number of Clusters

Select the optimal number of clusters (k) using methods like the Elbow Method or Silhouette Analysis.

#### Elbow Method

The Elbow Method involves running k-means clustering for a range of $k$ values and plotting the total within-cluster sum of squares. The "elbow" of the plot represents an optimal value for $k$ (a balance between precision and computational cost).

```{r Elbow plot}
# Compute total within-cluster sum of squares for different k values
set.seed(45)
wss <- sapply(1:10, function(k) {
  kmeans(selected_components, centers = k)$tot.withinss
})

# Plot the total within-cluster sum of squares
plot(1:10, wss, type = "b", xlab = "Number of Clusters", ylab = "Total Within-Cluster Sum of Squares",
     main = "Elbow Method")
```

based on the numbers, we might consider the point where the decrease starts to slow down, which could be around $k$= 4 or $k$=5.

#### Silhouette Analysis

Silhouette Analysis measures how similar an object is to its own cluster compared to other clusters. The silhouette score ranges from -1 to 1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.

```{r Shilouette scores}
library(cluster)

# Compute silhouette scores for different k values
set.seed(45)
silhouette_scores <- sapply(2:10, function(k) {
  cluster_result <- kmeans(selected_components, centers = k)
  silhouette_avg <- mean(silhouette(cluster_result$cluster, dist(selected_components))[, "sil_width"])
  silhouette_avg
})

# Plot the silhouette scores
plot(2:10, silhouette_scores, type = "b", xlab = "Number of Clusters", ylab = "Average Silhouette Width",
     main = "Silhouette Analysis")

```

The maximum silhouette score corresponds to 5 clusters (since the first value represents $k$=2)

### Step 3: Applying K-Means Algorithm

Utilize the k-means algorithm to divide the data into k clusters, focusing on the first three PCs. In R, this can be done with:

```{r Applying K-Means Algorithm}
#| eval: true
# Choose the number of clusters
k <- 5

# Perform k-means clustering
set.seed(45)
kmeans_result <- kmeans(selected_components, centers = k)
```

### Step 4: Interpret the Clusters

Investigate each cluster to understand the common traits within them. Interpretation requires a blend of data analysis and domain-specific knowledge.

```{r  Plot the first two principal components and color by cluster}
library(ggplot2)

# Create a data frame for plotting
plot_data <- data.frame(selected_components, cluster = as.factor(kmeans_result$cluster))

# Plot the first two principal components and color by cluster
ggplot(plot_data, aes(x = PC1, y = PC2, color = cluster)) +
  geom_point() +
  labs(title = "K-means Clustering on First Two Principal Components") +
  theme_minimal()

# Plot the first two principal components and color by cluster
ggplot(plot_data, aes(x = PC1, y = PC3, color = cluster)) +
  geom_point() +
  labs(title = "K-means Clustering on First and Third Principal Components") +
  theme_minimal()


```

An interactive 3D scatter plot below shows the first three principal components, colored by cluster. We can rotate the plot to view it from different angles, and you can hover over the points to see additional information.

```{r 3D scatter plot of the first-three PCs }
#| warning: false
#| messages: false

# Load the plotly package
library(plotly)
names_kab_kec<- scaled_df_complete_temp |> 
  select(c(nmkab, nmkec))

# Create a data frame for plotting
plot_data <- data.frame(selected_components, cluster = as.factor(kmeans_result$cluster)) |> bind_cols(names_kab_kec)

# Create the 3D scatter plot
plot_3d <- plot_ly(
  data = plot_data,
  x = ~ PC1,
  y = ~ PC2,
  z = ~ PC3,
  color = ~ cluster,
  type = "scatter3d",
  text = ~ paste("Kab./Kota:", nmkab, "<br>Kecamatan:", nmkec),
  mode = "markers"
) 
# Show the plot
plot_3d


```

### Step 5: Visualise the clusters into a map

<!-- ## Grids of box plots for each parameters -->

<!-- ```{r Box plots by typologies} -->

<!-- pca_eda <- tibble(df_pre_pca, cluster = as.factor(kmeans_result$cluster)) -->

<!-- library(ggplot2) -->

<!-- library(gridExtra) -->

<!-- # Convert the data frame from a wide to a long format   -->

<!-- pca_eda_long <- pca_eda|>  -->

<!--   tidyr::gather(key = "variable", value = "value", -cluster) -->

<!-- #Create a list of ggplot objects for each variable -->

<!-- plots <- lapply(unique(pca_eda_long$variable), function(var) { -->

<!--   ggplot(pca_eda_long[pca_eda_long$variable == var,], aes(x = cluster, y = value)) + -->

<!--     geom_boxplot() + -->

<!--     labs(title = var, x = "Cluster", y = "Value") + -->

<!--     theme_minimal() -->

<!-- }) -->

<!-- grid_plot <- do.call(gridExtra::grid.arrange, c(plots, ncol = 5)) -->

<!-- ggsave("output/boxplots.png", grid_plot, width = 20, height = 15) -->

<!-- grid_plot -->

<!-- ``` -->

```{r Interactive map}
#| warning: false
#| message: false
# Read the shapefile
desa <-
  st_read("data/INDO_DESA_2019/INDO_DESA_2019.shp", quiet = TRUE)

# Filter based on the 'kdprov' attribute
desa_sulsel <- desa |> filter(kdprov %in% 73)
desa_sulsel <- desa_sulsel |> dplyr::select(-c("kddesa", "iddesa"))
desa_sulsel_id_kec <-
  desa_sulsel |> select(nmkab, nmkec, kdprov, kdkab, kdkec) |>
  mutate(idkec_dum = paste0(kdprov, kdkab, kdkec, "a"))
rm(desa)

# add the cluster attribute to the desa_sulsel object
cluster_data <-
  scaled_df_complete_temp |># select(-c(nmkab, nmkec , nmprov,  sumber, kdprov, kdkab, kdkec, periode )) |>
  dplyr::select(idkec_dum ) |> bind_cols(tibble (cluster = kmeans_result$cluster))

clusters_sulsel <-
  desa_sulsel_id_kec |> left_join(cluster_data, by = "idkec_dum")

# Write the sf object to a shapefile
st_write(
  clusters_sulsel,
  "output/tipologi_5_kelas_sulsel.shp",
  append = TRUE,
  quiet = TRUE
)


# # Plot the SpatVector object, using the 'cluster' attribute for the fill color
# plot(desa_sulsel, "cluster", col=c("#E69F00", "#CC79A7", "#009E73", "#F0E442", "#0072B2"), lwd=0.1) # Assuming we have 5 clusters

# Create a color palette for the 'cluster' variable
pal <-
  colorFactor(palette = "Set1", domain = clusters_sulsel$cluster)

# Constructing the label string first
clusters_sulsel$label_content <- with(
  clusters_sulsel,
  paste0(
    "<strong>Kabupaten:</strong> ",
    nmkab,
    "<br>",
    "<strong>Kecamatan:</strong> ",
    nmkec,
    "<br>",
    "<strong>Cluster:</strong> ",
    cluster
  )
) |> lapply(htmltools::HTML)


# Create the leaflet map with HTML-rendered labels
leaflet(clusters_sulsel) |>
  addProviderTiles(providers$OpenStreetMap) |>
  addPolygons(
    fillColor = ~ pal(cluster),
    weight = 0.5,
    opacity = 1,
    color = "white",
    dashArray = "3",
    fillOpacity = 0.7,
    highlight = highlightOptions(
      weight = 5,
      color = "#666",
      dashArray = "",
      fillOpacity = 0.7,
      bringToFront = TRUE
    ),
    label = ~ label_content,
    labelOptions = labelOptions(noHide = FALSE,
                                direction = 'auto')
  ) |>
  addLegend(pal = pal,
            values = ~ cluster,
            title = "Cluster")
```

# Interpreting the cluster

## Define a nickname for each cluster

**Cluster 4:** Urban areas\
**Cluster 3:** Sub-urban areas\
**Cluster 5:** Sub-rural areas\
**Cluster 2:** Rural areas\
**Cluster 1:** Islands

```{r}
cluster_lookup <- tibble(cluster = as.numeric(c(4,3,5,2,1)), name = c("Urban areas", "Sub-urban areas", "Sub-rural areas", "Rural areas", "Islands"))

readr::write_csv(cluster_lookup, "output/cluster_names.csv")
cluster_lookup |> kable()
```

## Visualise the map
```{r}
clusters_sulsel_with_name <- clusters_sulsel |> right_join(cluster_lookup, by = "cluster")

pal <-
  colorFactor(palette = "Set1", domain = clusters_sulsel_with_name$name)
# Create the leaflet map with HTML-rendered labels
leaflet(clusters_sulsel_with_name) |>
  addProviderTiles(providers$OpenStreetMap) |>
  addPolygons(
    fillColor = ~ pal(name),
    weight = 0.5,
    opacity = 1,
    color = "white",
    dashArray = "3",
    fillOpacity = 0.7,
    highlight = highlightOptions(
      weight = 5,
      color = "#666",
      dashArray = "",
      fillOpacity = 0.7,
      bringToFront = TRUE
    ),
    label = ~ label_content,
    labelOptions = labelOptions(noHide = FALSE,
                                direction = 'auto')
  ) |>
  addLegend(pal = pal,
            values = ~ name,
            title = "Cluster")


```

## Create a table of mean, std deviation for each class

A table of summary with predictors as rows and each typology class as columns of mean and standard deviation.

```{r data wrangling}
df_interpretation <-
  df_raw |>
  dplyr::select(
    idkec_dum,
    distance_to_commodity_processing_factory,
    distance_to_deforestation,
    rasio_pt,
    indeks_bahaya_longsor,
    percentage_of_plantation_area_per_sub_district,
    indeks_bahaya_banjir,
    total_kk_berdasarkan_pengguna_dan_non_pengguna_listrik,
    distance_to_road,
    percentage_deforestation_area_size,
    buffer_to_500m_irigated_land,
    percentage_of_agricultural_area_small_holder_in_the_village,
    erosion_risk_t_ha_1_yr_1,
    distance_to_plantation,
    distance_to_burned_area,
    aridity_index,
    distance_to_forest,
    rasio_minimarket_swalayan,
    rasio_rs,
    arable_land_percent,
    distance_to_plantation_concession,
    distance_to_river,
    rasio_faskes_1,
    annual_mean_temp,
    temp_change,
    annual_mean_prec,
    prec_change) |> 
  mutate_at(vars(contains("percent")), ~ . * 100) |> 
  mutate_at(vars(contains("distance")), ~ . / 1000) |> 
  mutate(buffer_to_500m_irigated_land = buffer_to_500m_irigated_land*100)



polygon_sulsel_df <- clusters_sulsel  |> dplyr::select(-label_content) |> 
  left_join(df_interpretation, by = "idkec_dum") |> 
  tidyr::drop_na(cluster)


sulsel_df_long <- polygon_sulsel_df |> 
  sf::st_drop_geometry() |> 
  as_tibble() |>
  select(-(1:6)) |>
  pivot_longer(cols = -cluster,
               # Exclude the 'cluster' column
               names_to = "variable",
               # Name of the new column for variable names
               values_to = "value"    # Name of the new column for values
               ) |>  
  left_join(cluster_lookup, by = "cluster") |> 
  relocate(name, .after = cluster ) |> 
  dplyr::select(-cluster)

sulsel_df_long_summarised <- sulsel_df_long |>
  group_by(name, variable) |>
  summarise(mean = mean(value, na.rm = TRUE),
            std_dev = sd(value, na.rm = TRUE)) |> ungroup()


sulsel_df_wide <- sulsel_df_long_summarised |> 
   mutate(across(where(is.numeric), ~ round(., digits = 2))) |> 
  mutate(value_combined = paste0(mean, " (", std_dev, ")")) |> 
  select(-mean, -std_dev)  |> 
  pivot_wider(
    names_from = name, 
    values_from = value_combined,
    id_cols = variable
  ) 

lookup_units<- tibble::tribble(~variable, ~units,
                "annual_mean_prec", "mm/year",
                "annual_mean_temp", "deg C/year",
                "distance_to_commodity_processing_factory", "Km",
                "distance_to_deforestation", "Km",
                "rasio_pt", "kk/unit",
                "indeks_bahaya_longsor", "nilai indeks",
                "percentage_of_plantation_area_per_sub_district", "percent",
                "indeks_bahaya_banjir", "nilai indeks",
                "total_kk_berdasarkan_pengguna_dan_non_pengguna_listrik", "jumlah kk",
                "distance_to_road", "Km",
                "percentage_deforestation_area_size",  "percent",
                "buffer_to_500m_irigated_land", "% to kecamatan",
                "percentage_of_agricultural_area_small_holder_in_the_village", "percent",
                "erosion_risk_t_ha_1_yr_1", "(t/ha/yr)",
                "distance_to_plantation", "Km",
                "distance_to_burned_area", "Km",
                "aridity_index", "nilai indeks",
                "distance_to_forest", "Km",
                "rasio_minimarket_swalayan", "kk/unit",
                "rasio_rs", "kk/unit",
                "arable_land_percent",  "percent",
                "distance_to_plantation_concession", "Km",
                "distance_to_river", "Km",
                "rasio_faskes_1", "kk/unit",
                "prec_change", "mm/year",
                "temp_change", "deg C/year")

summary_table_sulsel <- sulsel_df_wide |> left_join(lookup_units, by = "variable")

summary_table_sulsel |> 
  readr::write_csv("output/intisari_tipologi_sulsel.csv")

```

```{r}
# Extract numeric values from a string
extract_numeric <- function(x) {
  # Extract the main numeric value (before any space or parenthesis)
  num_str <- gsub("([0-9.]+).*", "\\1", x)
  as.numeric(num_str)
}


# Function to create gradient color for each row
gradient_color_per_row <- function(data, colnames) {
  sapply(seq_len(nrow(data)), function(i) {
    values <- sapply(data[i, colnames], extract_numeric)
    rank_values <- rank(values, na.last = "keep")
    colorRampPalette(c("lightyellow", "lightblue", "darkblue"))(length(values))[rank_values]
  })
}

# Compute colors for each cell
colors <- gradient_color_per_row(summary_table_sulsel, colnames(summary_table_sulsel)[2:6])

# Apply the colors to the gt table
gt_table <- summary_table_sulsel %>%
  gt() %>%
  data_color(
    columns = 2:6,
    palette = colors
  ) %>%
  tab_options(
    container.overflow.x = TRUE,
    container.overflow.y = TRUE
  )
gt_table
```


# Warning

The analysis was conducted without in-depth knowledge of the region, so the results should be interpreted cautiously.
