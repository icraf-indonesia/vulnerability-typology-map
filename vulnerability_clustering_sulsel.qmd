---
title: "Typologies of Regions in South Sulawesi"
format: 
  html:
    code-overflow: wrap
    code-fold: true
    toc: true
    number-sections: true
editor: visual
project:
  type: website
  output-dir: docs
editor_options: 
  chunk_output_type: console
---

# Introduction

The objective here is to present a typology of climate vulnerability, identifying factors and indicators that determine a village's susceptibility to climate change.

## Pipeline

A flowchart includes steps in performing PCA. ![Steps in performing PCA](data/workflow.png)

## Loading the data

**Reading the Data:** We have an Excel file with lots of information. Each row in the file gives us details about a village, and the columns tell us about different factors that might make the village more or less vulnerable to climate change. There's also a special sheet in the Excel file that explains what all these factors mean, their unique IDs, and how they're measured.

### The main dataset `raw_df`

```{r Read raw data}
#| warning: false
library(readxl)
library(dplyr)
library(corrplot)
library(caret)
library(gtExtras)
library(naniar)
library(knitr)
library(kableExtra)

df_raw <-
  read_xlsx("data/analisa_tipologi_sulsel.xlsx",
            sheet = "fin_copas") |> 
  janitor::clean_names() |> dplyr::select(-c(idkec_2, idkec_3)) |> 
  mutate(across(-c(idkec_dum, sumber, nmprov, nmkab, nmkec), as.numeric))
```

```{r a subset of columns of the raw data}
# Select a subset of columns for clarity
df_selected <- df_raw|>
  dplyr::select(1,7,8,10,11, 12) |> head()

# Create the table using gt
df_selected|>
  gt()|>
  tab_header(
    title = "A sample of the Data"
  )|>
  cols_label(
    idkec_dum = "ID",
    nmkab = "District",
    nmkec = "Sub-district",
    distance_to_plantation = "Dist. to Plantation (m)",
    distance_to_road = "Dist. to Road (m)",
    distance_to_commodity_processing_factory = "Dist. to Comm.Proc Factory (m)"
  )|>
  fmt_number(
    columns = c(distance_to_plantation, distance_to_road, distance_to_commodity_processing_factory),
    decimals = 2
  )
```

Now we have a big table called `df_raw`. This table contains information like the name of the province, district, sub-district, and village. It also includes data about the village's area, population density, and lots of other numerical factors.

```{r transform and standardise the data}
# Remove unnecessary columns from the original dataframe 'df_raw'
# The 'dplyr::select()' function is used to exclude these columns
df_gt <- df_raw |> 
  dplyr::select(-c(idkec_dum, kdprov, sumber, nmprov, nmkab, nmkec, periode, kdkab, kdkec, prec_change))

df_col_unstandardise <- df_raw |> dplyr::select(prec_change)

# Add a small constant (0.001) to every value in 'df_gt'
# This is often done to allow for log transformations of data that includes zeros
df_gt_plus <- df_gt + 0.001



# Log-transform and scale the data
# 1. 'as_tibble()' converts the dataframe to a tibble for easier manipulation
# 2. 'mutate_all(.funs = log10)' applies the base-10 logarithm to all columns
# 3. 'scale()' standardizes each column so that it has mean=0 and sd=1
scaled_df <-
  df_gt_plus |> as_tibble() |> mutate_all(.funs = log10) |> bind_cols(df_col_unstandardise) |>  scale(center = TRUE, scale = TRUE)

# Add back the columns that were removed earlier to create a complete, scaled dataframe
# 'bind_cols()' binds the selected columns from 'df_raw' and the scaled columns from 'scaled_df'
scaled_df_complete <- df_raw |> 
  dplyr::select(c(idkec_dum, kdprov, sumber, nmprov, nmkab, nmkec, periode, kdkab, kdkec)) |>  
  bind_cols(scaled_df)

# Remove the column 'korban_jiwa_kebakaran_hutan_dan_lahan_2018_2019' from the complete, scaled dataframe
scaled_df_complete_temp <- scaled_df_complete |> 
  dplyr::select(-korban_jiwa_kebakaran_hutan_dan_lahan_2018_2019)

```

# Preparing the data

Before diving into the PCA, let's ensure that the data meets the necessary requirements.

## Check for missing values

```{r Remove incomplete kecamatans}
# Check for missing values
missing_data <- sapply(scaled_df_complete_temp, function(x) sum(is.na(x)))

# print missing data\
scaled_df_complete_temp|> filter_all(any_vars(is.na(.))) |>   dplyr::select("ID Kecamatan" = 1, "Nama Kabupaten" = 5, "Nama Kecamatan " = 6) |> kable(caption = "Kecamatan dengan data tidak lengkap")

#remove missing data
scaled_df_complete_temp <- scaled_df_complete_temp[complete.cases(scaled_df_complete_temp), ]

```

## Exclude identifiers

```{r display the data ready for PCA}
#| warning: false
df_pre_pca <- scaled_df_complete_temp |> 
  select(-c(idkec_dum, sumber, kdprov, nmprov, nmkab, nmkec, periode, kdkab, kdkec))

glimpse(df_pre_pca) |> kbl(caption = "Input data for a PCA Analysis") |>   kable_paper()|>
  scroll_box(width = "1000px", height = "500px")
```

<!-- ## Exclude constant or near constant columns -->

<!-- In Principal Component Analysis (PCA), you want to reduce the dimensionality of your dataset to uncover patterns and make visualization easier. However, if some variables (columns) have constant or near-constant values across the observations, they can't contribute to explaining the variability in your data. Such columns should be removed before performing PCA. Here's some code that identify these columns: -->

No missing data is found. Handle missing values if any, possibly through imputation

## Identify multi-collinear variables

Compute the correlation matrix and identify highly correlated variables:

```{r display correlation matrix}
#df_pre_pca_abb <- df_pre_pca
# Abbreviate column names using R's built-in abbreviate function
#abbrev_colnames <- abbreviate(colnames(df_pre_pca_abb))

# Assign the abbreviated names back to the dataframe
#colnames(df_pre_pca_abb) <- abbrev_colnames

# Recalculate and plot the correlation matrix
cor_matrix <- cor(df_pre_pca) |> round(digits = 2)

# Convert the matrix to a data frame
cor_matrix_df <- as.data.frame(cor_matrix)

# Add row names as a new column
cor_matrix_df$Predictors <- rownames(cor_matrix)

# Move the 'RowNames' column to the first position
cor_matrix_df <- cor_matrix_df|> select(Predictors, everything())



# Create the gt table
cor_matrix_df|>
  gt()|>
  data_color(
    columns = -Predictors,  # Exclude the 'RowNames' column from colorization
    colors = scales::col_numeric(
      palette = c("darkred", "white", "darkblue"),
      domain = c(-1, 1)
    )
  ) |> tab_options(container.overflow.x = TRUE, container.overflow.y = TRUE)


```

```{r display multi-collinear variables}
# Set a threshold value for correlation
threshold <- 0.8

# Find the indices of the variables that are highly correlated according to the specified threshold in the correlation matrix 'cor_matrix.'
# This will be used to remove the highly correlated variables from 'df_pre_pca.'
highly_correlated <- findCorrelation(cor(df_pre_pca), cutoff = threshold, names = FALSE)

# If you also need the names of the highly correlated variables, you can extract them using the indices.
highly_correlated_names <- colnames(cor(df_pre_pca))[highly_correlated]


# # Remove the columns from 'df_pre_pca' that correspond to the indices of the highly correlated variables.
# df_pre_pca <- df_pre_pca |> dplyr::select(-highly_correlated_names[[1]])
# Create a data frame from highly_correlated_names and threshold
highly_correlated_df <- data.frame(
  Predictor = highly_correlated_names
 # Threshold = threshold
)

# Create a gt table
highly_correlated_df|>
  gt()|>
  tab_header(
    title = "Highly Correlated Predictors",
    subtitle = paste("Predictors with correlation above", threshold)
  )|>
  cols_label(
    Predictor = "Predictor Name"
   # Threshold = "Correlation Threshold"
  )

```

# Exploratory data analysis (EDA)

Visualize the data and examine any patterns, distributions, or outliers. In Principal Component Analysis (PCA), you want to reduce the dimensionality of your dataset to uncover patterns and make visualization easier. However, if some variables (columns) have constant or near-constant values across the observations, they can't contribute to explaining the variability in your data. Such columns are advised to be removed before performing PCA.

```{r}
#| warning: false

# Pivot the 'scaled_df_complete_temp' dataframe to long format
# Exclude certain columns from being pivoted and specify the new column names for 'variable' and 'value'
df_long <- scaled_df_complete_temp |> 
  dplyr::select(1:9) |> bind_cols(df_pre_pca) |> 
  tidyr::pivot_longer(
    cols = -c(idkec_dum, sumber, kdprov, nmprov, nmkab, nmkec, periode, kdkab, kdkec),
    names_to = "variable",
    values_to = "value"
  )

# Select only the 'variable' and 'value' columns from the long-form data
# Remove unwanted columns and group by 'variable' to prepare for summarization
gt_tab <- df_long |>
  dplyr::select(-c(idkec_dum, sumber, kdprov, nmprov, nmkab, nmkec, periode, kdkab, kdkec)) |>
  group_by(variable) 

# Calculate summary statistics for each group (each 'variable')
# Also, keep the original 'value' data in a list column called 'value_data'
# Use '.groups = "drop"' to return a regular dataframe rather than a grouped one
gt_stat <- gt_tab |> 
  summarise(
    min = min(value, na.rm = TRUE),
    max = max(value, na.rm = TRUE),
    median = median(value, na.rm = TRUE),
    #mean = mean(value, na.rm = TRUE),
    #sd = sd(value, na.rm = TRUE),
    value_data = list(value),
    .groups = "drop"
  )

# Create a GT table with a density plot column
# The density plot is generated from the 'value_data' list column
# Customize the appearance of the density plot and format the number columns
gt_stat |>
  gt::gt() |> 
  gtExtras::gt_plt_dist(
    value_data,
    type = "density",
    line_color = "blue",
    fill_color = "red"
  ) |> gt::fmt_number(columns = min:median, decimals = 1) |> 
  cols_label(
    variable = "Predictors",
    min = "Minimum",
    max = "Maximum",
    median = "Median",
    value_data = "Density"
  ) 

```

```{r}
## Boxplot
# Histograms
# boxplot(df_pre_pca)

```

## Density plots

```{r}
#| warning: false
#| include: false

# Start the PNG device
png("output/density_plots.png", width = 2000, height = 2800)

# Set up the plot layout
par(mfrow = c(7, 7))

# Define the font size
font_size <- 4

# Loop through the columns of the data frame
for (i in 1:ncol(df_pre_pca)) {
  # Get the current variable
  variable <- df_pre_pca[, i]
  
  # Create a density plot for the variable with larger font size
  plot(density(variable[[1]]), main = "", xlab = "", ylab = "", cex.axis = font_size, cex.lab = font_size)
  
  # Add the title using the column name with larger font size
  title(main = colnames(df_pre_pca)[i], cex.main = font_size)
}

# Close the PNG device
dev.off()
```

# PCA Analysis

```{r perform pca}
# Perform PCA
pca_result <- prcomp(df_pre_pca)
```

```{r pca summary}
# Assuming pca_result is your prcomp object
pca_summary <- summary(pca_result)$importance |> round(digits = 2)



# Convert to tibble and remove row names
pca_summary_tibble <- tibble::rownames_to_column(as.data.frame(pca_summary), var = "Components")

# Create gt table
pca_summary_tibble|>  
  gt()|>
  tab_header(
    title = "Principal Component Analysis Summary",
    subtitle = "Importance of components") |> 
    opt_align_table_header(align = "left")
```

# Future Steps

## Reducing Variables in PCA Analysis

By selecting variables based on the significance of loadings for the top principal components, we can focus on the ones that contribute most to the data's variance. This reduction simplifies the analysis and emphasizes the most influential factors.

## Understanding the Principal Components (PCs)

<!-- ### The Biplot -->

<!-- Biplots graphically represent the relationship between observations and variables, providing insight into the PCs. -->

<!-- ```{r} -->

<!-- biplot(pca_result) -->

<!-- ``` -->

### The Scree Plot

A scree plot visualizes the proportion of variance explained by each PC. The first few PCs are the main contributors, while the rest have minimal influence.

```{r Create a scree plot}
# Visualize the PCA result using a scree plot to see the variance explained by each principal component
plot(pca_result, type = "l", main = "Scree Plot")

```

### A Look at Loadings

Loadings in PCA reveal how original variables influence each Principal Component. High absolute values indicate strong influence, while the sign indicates the direction. Understanding these can uncover underlying themes, such as climate factors.

#### The importance of the factors for each principal component

```{r visualise loadings}
# Extract the loadings for the desired number of principal components
num_pc <- 5
loadings <- pca_result$rotation[, 1:num_pc]

# Function to plot ordered loadings
plot_ordered_loadings <- function(loadings, pc_num) {
  ordered_indices <- order(-abs(loadings[, pc_num]), decreasing = TRUE)
  
  # Determine the colours based on the sign of the values
  bar_colours <- ifelse(loadings[ordered_indices, pc_num] < 0, "red", "blue")
  
  barplot(abs(loadings[ordered_indices, pc_num]), horiz = TRUE,
          main = paste("Loadings for PC", pc_num),
          names.arg = rownames(loadings)[ordered_indices],
          las = 1, cex.names = 0.7, xlim = c(0, 0.5), col = bar_colours)
}

# Set up the plotting area with adjusted margins
par(mar = c(5, 25, 4, 2))

# Loop over each principal component and plot
for (i in 1:num_pc) {
  plot_ordered_loadings(loadings, i)
}


```

To identify the factors that are consistently least influential across the first three principal components, we can look at the absolute values of the loadings. Factors with small absolute loadings are less influential. Knowledge of the specific field aids in interpreting PCs. Principal Components can be complex and may resist straightforward interpretation.

## Classifying Data with K-Means Clustering and Principal Components

### Step 1: Selecting the Principal Components

Start by using the top five Principal Components from PCA, which capture the core variances and correlations in your data.

```{r select the first 5 PCs}
# Extract the first five principal components
selected_components <- pca_result$x[, 1:5]
```

### Step 2: Determine the Number of Clusters

Select the optimal number of clusters (k) using methods like the Elbow Method or Silhouette Analysis.

#### Elbow Method

The Elbow Method involves running k-means clustering for a range of $k$ values and plotting the total within-cluster sum of squares. The "elbow" of the plot represents an optimal value for $k$ (a balance between precision and computational cost).

```{r Elbow plot}
# Compute total within-cluster sum of squares for different k values
set.seed(45)
wss <- sapply(1:10, function(k) {
  kmeans(selected_components, centers = k)$tot.withinss
})

# Plot the total within-cluster sum of squares
plot(1:10, wss, type = "b", xlab = "Number of Clusters", ylab = "Total Within-Cluster Sum of Squares",
     main = "Elbow Method")
```

based on the numbers, you might consider the point where the decrease starts to slow down, which could be around $k$= 4 or $k$=5.

#### Silhouette Analysis

Silhouette Analysis measures how similar an object is to its own cluster compared to other clusters. The silhouette score ranges from -1 to 1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.

```{r Shilouette scores}
library(cluster)

# Compute silhouette scores for different k values
set.seed(45)
silhouette_scores <- sapply(2:10, function(k) {
  cluster_result <- kmeans(selected_components, centers = k)
  silhouette_avg <- mean(silhouette(cluster_result$cluster, dist(selected_components))[, "sil_width"])
  silhouette_avg
})

# Plot the silhouette scores
plot(2:10, silhouette_scores, type = "b", xlab = "Number of Clusters", ylab = "Average Silhouette Width",
     main = "Silhouette Analysis")

```

The maximum silhouette score is 0.3428859, which corresponds to 5 clusters (since the first value represents $k$=2)

### Step 3: Applying K-Means Algorithm

Utilize the k-means algorithm to divide the data into k clusters, focusing on the first three PCs. In R, this can be done with:

```{r Applying K-Means Algorithm}
#| eval: true
# Choose the number of clusters
k <- 5

# Perform k-means clustering
set.seed(45)
kmeans_result <- kmeans(selected_components, centers = k)
```

### Step 4: Interpret the Clusters

Investigate each cluster to understand the common traits within them. Interpretation requires a blend of data analysis and domain-specific knowledge.

```{r  Plot the first two principal components and color by cluster}
library(ggplot2)

# Create a data frame for plotting
plot_data <- data.frame(selected_components, cluster = as.factor(kmeans_result$cluster))

# Plot the first two principal components and color by cluster
ggplot(plot_data, aes(x = PC1, y = PC2, color = cluster)) +
  geom_point() +
  labs(title = "K-means Clustering on First Two Principal Components") +
  theme_minimal()

# Plot the first two principal components and color by cluster
ggplot(plot_data, aes(x = PC1, y = PC3, color = cluster)) +
  geom_point() +
  labs(title = "K-means Clustering on First and Third Principal Components") +
  theme_minimal()


```

An interactive 3D scatter plot below shows the first three principal components, colored by cluster. We can rotate the plot to view it from different angles, and you can hover over the points to see additional information.

```{r 3D scatter plot of the first-three PCs }

# Load the plotly package
library(plotly)
names_kab_kec<- scaled_df_complete_temp |> 
  select(c(nmkab, nmkec))

# Create a data frame for plotting
plot_data <- data.frame(selected_components, cluster = as.factor(kmeans_result$cluster)) |> bind_cols(names_kab_kec)

# Create the 3D scatter plot
plot_3d <- plot_ly(
  data = plot_data,
  x = ~ PC1,
  y = ~ PC2,
  z = ~ PC3,
  color = ~ cluster,
  type = "scatter3d",
  text = ~ paste("Kab./Kota:", nmkab, "<br>Kecamatan:", nmkec),
  mode = "markers"
) 
# Show the plot
plot_3d


```

# Exploratory data analysis

## Grids of box plots for each parameters

```{r Box plots by typologies}
pca_eda <- tibble(df_pre_pca, cluster = as.factor(kmeans_result$cluster))

library(ggplot2)
library(gridExtra)

# Convert the data frame from a wide to a long format  
pca_eda_long <- pca_eda|> 
  tidyr::gather(key = "variable", value = "value", -cluster)

#Create a list of ggplot objects for each variable
plots <- lapply(unique(pca_eda_long$variable), function(var) {
  ggplot(pca_eda_long[pca_eda_long$variable == var,], aes(x = cluster, y = value)) +
    geom_boxplot() +
    labs(title = var, x = "Cluster", y = "Value") +
    theme_minimal()
})
grid_plot <- do.call(gridExtra::grid.arrange, c(plots, ncol = 5))

ggsave("output/boxplots.png", grid_plot, width = 20, height = 15)
grid_plot
```

## Visualise the clusters into a map

# Read a map then attach

```{r}
library(terra)
library(sf)
# Read the shapefile
desa <- st_read("data/INDO_DESA_2019/INDO_DESA_2019.shp")

# Filter based on the 'kdprov' attribute
desa_sulsel <- desa |> filter(kdprov %in% 73)
desa_sulsel <- desa_sulsel |> dplyr::select(-c("kddesa", "iddesa"))
desa_sulsel_id_kec <- desa_sulsel |> select(nmkab,nmkec,kdprov,kdkab,kdkec) |> 
  mutate(idkec_dum = paste0(kdprov,kdkab,kdkec, "a") )
rm(desa)

# add the cluster attribute to the desa_sulsel object
cluster_data<- scaled_df_complete_temp |> select(-c(nmkab, nmkec ,nmprov ,  sumber, kdprov,kdkab,kdkec)) |> 
  dplyr::select(1:9) |> bind_cols(tibble (cluster = kmeans_result$cluster))

clusters_sulsel<- desa_sulsel_id_kec |> left_join(cluster_data, by = "idkec_dum")

# Write the sf object to a shapefile
st_write(clusters_sulsel, "output/tipologi_5_kelas.shp", append = TRUE)

# # Plot the SpatVector object, using the 'cluster' attribute for the fill color
# plot(desa_sulsel, "cluster", col=c("#E69F00", "#CC79A7", "#009E73", "#F0E442", "#0072B2"), lwd=0.1) # Assuming we have 5 clusters

library(leaflet)
# Create a color palette for the 'cluster' variable
pal <- colorFactor(palette = "Set1", domain = clusters_sulsel$cluster)

# Constructing the label string first
clusters_sulsel$label_content <- with(clusters_sulsel, 
                                      paste0("<strong>Kabupaten:</strong> ", nmkab, "<br>",
                                             "<strong>Kecamatan:</strong> ", nmkec, "<br>",
                                             "<strong>Cluster:</strong> ", cluster)) |> lapply(htmltools::HTML)

# Check the first few rows to ensure the label is being constructed correctly
head(clusters_sulsel$label_content)

# Create the leaflet map with HTML-rendered labels
leaflet(clusters_sulsel) |> 
  addProviderTiles(providers$OpenStreetMap)|>
  addPolygons(
    fillColor = ~pal(cluster),
    weight = 0.5,
    opacity = 1,
    color = "white",
    dashArray = "3",
    fillOpacity = 0.7,
    highlight = highlightOptions(
      weight = 5,
      color = "#666",
      dashArray = "",
      fillOpacity = 0.7,
      bringToFront = TRUE
    ),
    label = ~label_content,
    labelOptions = labelOptions(
      noHide = FALSE,
      direction = 'auto'
    )
  )|>
  addLegend(pal = pal, values = ~cluster, title = "Cluster")


```

# Warning

-   **Lack of Data Understanding**: The analysis was conducted without in-depth knowledge of the data, so the results should be interpreted cautiously.
