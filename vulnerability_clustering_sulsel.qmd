---
title: "vulnerability_clusters"
format: 
  html:
    code-overflow: wrap
    code-fold: true
    toc: true
    number-sections: true
editor: visual
project:
  type: website
  output-dir: docs
editor_options: 
  chunk_output_type: console
---

# Introduction

The objective here is to present a typology of climate vulnerability, identifying factors and indicators that determine a village's susceptibility to climate change.

## Pipeline

A flowchart includes steps in performing PCA.

## Loading the data

**Reading the Data:** We have an Excel file with lots of information. Each row in the file gives us details about a village, and the columns tell us about different factors that might make the village more or less vulnerable to climate change. There's also a special sheet in the Excel file that explains what all these factors mean, their unique IDs, and how they're measured.

### The main dataset `raw_df`

```{r}
#| warning: false
library(readxl)
library(dplyr)
library(corrplot)
library(caret)
library(gtExtras)
library(naniar)

df_raw <-
  read_xlsx("data/analisa_tipologi_sulsel.xlsx",
            sheet = "fin_copas") |> 
  janitor::clean_names() |> dplyr::select(-c(idkec_2, idkec_3)) |> 
  mutate(across(-c(idkec_dum, sumber, nmprov, nmkab, nmkec), as.numeric))

head(df_raw)
```

Now we have a big table called `df_raw`. This table contains information like the name of the province, district, sub-district, and village. It also includes data about the village's area, population density, and lots of other numerical factors.

```{r}
# Remove unnecessary columns from the original dataframe 'df_raw'
# The 'dplyr::select()' function is used to exclude these columns
df_gt <- df_raw |> 
  dplyr::select(-c(idkec_dum, kdprov, sumber, nmprov, nmkab, nmkec, periode, kdkab, kdkec, prec_change))

df_col_unstandardise <- df_raw |> dplyr::select(prec_change)

# Add a small constant (0.001) to every value in 'df_gt'
# This is often done to allow for log transformations of data that includes zeros
df_gt_plus <- df_gt + 0.001



# Log-transform and scale the data
# 1. 'as_tibble()' converts the dataframe to a tibble for easier manipulation
# 2. 'mutate_all(.funs = log10)' applies the base-10 logarithm to all columns
# 3. 'scale()' standardizes each column so that it has mean=0 and sd=1
scaled_df <- df_gt_plus |> as_tibble() |> mutate_all(.funs = log10) |>bind_cols(df_col_unstandardise) |>  scale(center = TRUE, scale = TRUE)

# Add back the columns that were removed earlier to create a complete, scaled dataframe
# 'bind_cols()' binds the selected columns from 'df_raw' and the scaled columns from 'scaled_df'
scaled_df_complete <- df_raw |> 
  dplyr::select(c(idkec_dum, kdprov, sumber, nmprov, nmkab, nmkec, periode, kdkab, kdkec)) |>  
  bind_cols(scaled_df)

# Remove the column 'korban_jiwa_kebakaran_hutan_dan_lahan_2018_2019' from the complete, scaled dataframe
scaled_df_complete_temp <- scaled_df_complete |> 
  dplyr::select(-korban_jiwa_kebakaran_hutan_dan_lahan_2018_2019)

```

# Preparing the data

Before diving into the PCA, let's ensure that the data meets the necessary requirements.

## Check for missing values

```{r}
# Check for missing values
missing_data <- sapply(scaled_df_complete_temp, function(x) sum(is.na(x)))
print(missing_data)

scaled_df_complete_temp %>% filter_all(any_vars(is.na(.)))

#remove missing data
scaled_df_complete_temp <- scaled_df_complete_temp[complete.cases(scaled_df_complete_temp), ]
```

## Exclude identifiers

```{r}
#| warning: false
df_pre_pca <- scaled_df_complete_temp |> 
  select(-c(idkec_dum, sumber, kdprov, nmprov, nmkab, nmkec, periode, kdkab, kdkec))

glimpse(df_pre_pca)
```

<!-- ## Exclude constant or near constant columns -->

<!-- In Principal Component Analysis (PCA), you want to reduce the dimensionality of your dataset to uncover patterns and make visualization easier. However, if some variables (columns) have constant or near-constant values across the observations, they can't contribute to explaining the variability in your data. Such columns should be removed before performing PCA. Here's some code that identify these columns: -->

<!-- ```{r} -->

<!-- # Set the threshold -->

<!-- threshold <- 0.025 -->

<!-- # Find the standard deviation for each numeric column -->

<!-- std_devs <- apply(df_pre_pca, MARGIN =  2, FUN = sd, na.rm=TRUE) # Excluding the first 5 columns that are non-numeric -->

<!-- # Identify the columns with standard deviation below the threshold -->

<!-- near_constant_columns <- names(std_devs)[std_devs < threshold] -->

<!-- ``` -->

<!-- ```{r} -->

<!-- # Exclude those columns from the dataset -->

<!-- df_pre_pca <- df_pre_pca[, !(names(df_pre_pca) %in% near_constant_columns)] -->

<!-- names(df_pre_pca) -->

<!-- ``` -->

<!-- ## log10 transformation -->

<!-- ```{r} -->

<!-- # Apply the log10 transformation, adding 1 to handle zeros -->

<!-- df_pre_pca <- df_pre_pca |> mutate(across(-T06, ~log10(.x))) #ignore T06 as it has been scaled -->

<!-- ``` -->

No missing data is found. Handle missing values if any, possibly through imputation

## Scale the data

## Identify and remove multi-collinear variables

Identifying and removing multicollinearity is an essential step before performing PCA, as PCA assumes that the features are linearly independent. Compute the correlation matrix and identify highly correlated variables:

```{r}
#df_pre_pca_abb <- df_pre_pca
# Abbreviate column names using R's built-in abbreviate function
#abbrev_colnames <- abbreviate(colnames(df_pre_pca_abb))

# Assign the abbreviated names back to the dataframe
#colnames(df_pre_pca_abb) <- abbrev_colnames

# Recalculate and plot the correlation matrix
cor_matrix <- cor(df_pre_pca) |> round(digits = 2)

# Convert the matrix to a data frame
cor_matrix_df <- as.data.frame(cor_matrix)

# Add row names as a new column
cor_matrix_df$Predictors <- rownames(cor_matrix)

# Move the 'RowNames' column to the first position
cor_matrix_df <- cor_matrix_df %>% select(Predictors, everything())



# Create the gt table
cor_matrix_df %>%
  gt() %>%
  data_color(
    columns = -Predictors,  # Exclude the 'RowNames' column from colorization
    colors = scales::col_numeric(
      palette = c("darkred", "white", "darkblue"),
      domain = c(-1, 1)
    )
  )

#corrplot(cor_matrix, method="circle", tl.cex = 0.5, type = "lower")
```

**Systematically Remove Multicollinear Variables**: decide on a correlation threshold that indicates multicollinearity (e.g., 0.8) and then manually or programmatically remove one variable from each pair or group that exceeds this threshold. The removed variables are:

```{r}
# Set a threshold value for correlation
threshold <- 0.8

# Find the indices of the variables that are highly correlated according to the specified threshold in the correlation matrix 'cor_matrix.'
# This will be used to remove the highly correlated variables from 'df_pre_pca.'
highly_correlated <- findCorrelation(cor(df_pre_pca), cutoff = threshold, names = FALSE)

# If you also need the names of the highly correlated variables, you can extract them using the indices.
highly_correlated_names <- colnames(cor(df_pre_pca))[highly_correlated]


# # Remove the columns from 'df_pre_pca' that correspond to the indices of the highly correlated variables.
# df_pre_pca <- df_pre_pca |> dplyr::select(-highly_correlated_names[[1]])
# Create a data frame from highly_correlated_names and threshold
highly_correlated_df <- data.frame(
  Predictor = highly_correlated_names
 # Threshold = threshold
)

# Create a gt table
highly_correlated_df %>%
  gt() %>%
  tab_header(
    title = "Highly Correlated Predictors",
    subtitle = paste("Predictors with correlation above", threshold)
  ) %>%
  cols_label(
    Predictor = "Predictor Name"
   # Threshold = "Correlation Threshold"
  )


```

# Exploratory data analysis (EDA)

Visualize the data and examine any patterns, distributions, or outliers.

```{r}
#| warning: false

# Pivot the 'scaled_df_complete_temp' dataframe to long format
# Exclude certain columns from being pivoted and specify the new column names for 'variable' and 'value'
df_long <- scaled_df_complete_temp |> 
  dplyr::select(1:9) |> bind_cols(df_pre_pca) |> 
  tidyr::pivot_longer(
    cols = -c(idkec_dum, sumber, kdprov, nmprov, nmkab, nmkec, periode, kdkab, kdkec),
    names_to = "variable",
    values_to = "value"
  )

# Select only the 'variable' and 'value' columns from the long-form data
# Remove unwanted columns and group by 'variable' to prepare for summarization
gt_tab <- df_long |>
  dplyr::select(-c(idkec_dum, sumber, kdprov, nmprov, nmkab, nmkec, periode, kdkab, kdkec)) |>
  group_by(variable) 

# Calculate summary statistics for each group (each 'variable')
# Also, keep the original 'value' data in a list column called 'value_data'
# Use '.groups = "drop"' to return a regular dataframe rather than a grouped one
gt_stat <- gt_tab |> 
  summarise(
    min = min(value, na.rm = TRUE),
    max = max(value, na.rm = TRUE),
    median = median(value, na.rm = TRUE),
    #mean = mean(value, na.rm = TRUE),
    #sd = sd(value, na.rm = TRUE),
    value_data = list(value),
    .groups = "drop"
  )

# Create a GT table with a density plot column
# The density plot is generated from the 'value_data' list column
# Customize the appearance of the density plot and format the number columns
gt_stat |>
  gt::gt() |> 
  gtExtras::gt_plt_dist(
    value_data,
    type = "density",
    line_color = "blue",
    fill_color = "red"
  ) |> gt::fmt_number(columns = min:median, decimals = 1)

```

```{r}
## Boxplot
# Histograms

# boxplot(df_pre_pca)

```

## Density plots

```{r}

# Start the PNG device
png("output/density_plots.png", width = 2000, height = 2800)

# Set up the plot layout
par(mfrow = c(7, 7))

# Define the font size
font_size <- 4

# Loop through the columns of the data frame
for (i in 1:ncol(df_pre_pca)) {
  # Get the current variable
  variable <- df_pre_pca[, i]
  
  # Create a density plot for the variable with larger font size
  plot(density(variable[[1]]), main = "", xlab = "", ylab = "", cex.axis = font_size, cex.lab = font_size)
  
  # Add the title using the column name with larger font size
  title(main = colnames(df_pre_pca)[i], cex.main = font_size)
}

# Close the PNG device
dev.off()
```

# PCA Analysis

```{r}
# Perform PCA
pca_result <- prcomp(df_pre_pca)
```

```{r}
# Assuming pca_result is your prcomp object
pca_summary <- summary(pca_result)$importance |> round(digits = 2)



# Convert to tibble and remove row names
pca_summary_tibble <- tibble::rownames_to_column(as.data.frame(pca_summary), var = "Components")

# Create gt table
pca_summary_tibble %>%  
  gt() %>%
  tab_header(
    title = "Principal Component Analysis Summary",
    subtitle = "Importance of components"
  )
```

# Future Steps

## Reducing Variables in PCA Analysis

By selecting variables based on the significance of loadings for the top principal components, we can focus on the ones that contribute most to the data's variance. This reduction simplifies the analysis and emphasizes the most influential factors.

## Understanding the Principal Components (PCs)

### The Biplot

Biplots graphically represent the relationship between observations and variables, providing insight into the PCs.

```{r}
biplot(pca_result)
```

### The Scree Plot

A scree plot visualizes the proportion of variance explained by each PC. The first few PCs are the main contributors, while the rest have minimal influence.

```{r}
# Visualize the PCA result using a scree plot to see the variance explained by each principal component
plot(pca_result, type = "l")

```

### A Look at Loadings

Loadings in PCA reveal how original variables influence each Principal Component. High absolute values indicate strong influence, while the sign indicates the direction. Understanding these can uncover underlying themes, such as climate factors.

#### The importance of the factors for each principal component

```{r}
# Extract the loadings for the desired number of principal components
num_pc <- 13
loadings <- pca_result$rotation[, 1:num_pc]

# Function to plot ordered loadings
plot_ordered_loadings <- function(loadings, pc_num) {
  ordered_indices <- order(-abs(loadings[, pc_num]), decreasing = TRUE)
  
  # Determine the colours based on the sign of the values
  bar_colours <- ifelse(loadings[ordered_indices, pc_num] < 0, "red", "blue")
  
  barplot(abs(loadings[ordered_indices, pc_num]), horiz = TRUE,
          main = paste("Loadings for PC", pc_num),
          names.arg = rownames(loadings)[ordered_indices],
          las = 1, cex.names = 0.7, xlim = c(0, 0.5), col = bar_colours)
}

# Set up the plotting area with adjusted margins
par(mar = c(5, 25, 4, 2))

# Loop over each principal component and plot
for (i in 1:num_pc) {
  plot_ordered_loadings(loadings, i)
}


```

To identify the factors that are consistently least influential across the first three principal components, we can look at the absolute values of the loadings. Factors with small absolute loadings are less influential. Knowledge of the specific field aids in interpreting PCs. Principal Components can be complex and may resist straightforward interpretation.

## Classifying Data with K-Means Clustering and Principal Components

### Step 1: Selecting the Principal Components

Start by using the top three Principal Components from PCA, which capture the core variances and correlations in your data.

```{r}
# Extract the first three principal components
selected_components <- pca_result$x[, 1:13]
```

### Step 2: Determine the Number of Clusters

Select the optimal number of clusters (k) using methods like the Elbow Method or Silhouette Analysis.

#### Elbow Method

The Elbow Method involves running k-means clustering for a range of $k$ values and plotting the total within-cluster sum of squares. The "elbow" of the plot represents an optimal value for $k$ (a balance between precision and computational cost).

```{r}
# Compute total within-cluster sum of squares for different k values
wss <- sapply(1:10, function(k) {
  kmeans(selected_components, centers = k)$tot.withinss
})

# Plot the total within-cluster sum of squares
plot(1:10, wss, type = "b", xlab = "Number of Clusters", ylab = "Total Within-Cluster Sum of Squares",
     main = "Elbow Method")
```

based on the numbers, you might consider the point where the decrease starts to slow down, which could be around $k$= 4 or $k$=5.

#### Silhouette Analysis

Silhouette Analysis measures how similar an object is to its own cluster compared to other clusters. The silhouette score ranges from -1 to 1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.

```{r}
library(cluster)

# Compute silhouette scores for different k values
silhouette_scores <- sapply(2:10, function(k) {
  cluster_result <- kmeans(selected_components, centers = k)
  silhouette_avg <- mean(silhouette(cluster_result$cluster, dist(selected_components))[, "sil_width"])
  silhouette_avg
})

# Plot the silhouette scores
plot(2:10, silhouette_scores, type = "b", xlab = "Number of Clusters", ylab = "Average Silhouette Width",
     main = "Silhouette Analysis")

```

The maximum silhouette score is 0.3428859, which corresponds to 5 clusters (since the first value represents $k$=2)

### Step 3: Applying K-Means Algorithm

Utilize the k-means algorithm to divide the data into k clusters, focusing on the first three PCs. In R, this can be done with:

```{r}
#| eval: true
# Choose the number of clusters
k <- 5

# Perform k-means clustering
kmeans_result <- kmeans(selected_components, centers = k)
```

### Step 4: Interpret the Clusters

Investigate each cluster to understand the common traits within them. Interpretation requires a blend of data analysis and domain-specific knowledge.

```{r}
library(ggplot2)

# Create a data frame for plotting
plot_data <- data.frame(selected_components, cluster = as.factor(kmeans_result$cluster))

# Plot the first two principal components and color by cluster
ggplot(plot_data, aes(x = PC1, y = PC2, color = cluster)) +
  geom_point() +
  labs(title = "K-means Clustering on First Two Principal Components") +
  theme_minimal()

```

An interactive 3D scatter plot below shows the first three principal components, colored by cluster. We can rotate the plot to view it from different angles, and you can hover over the points to see additional information.

```{r}

# Load the plotly package
library(plotly)

# Create a data frame for plotting
plot_data <- data.frame(selected_components, cluster = as.factor(kmeans_result$cluster))

# Create the 3D scatter plot
plot_3d <- plot_ly(data = plot_data, x = ~PC1, y = ~PC2, z = ~PC3, color = ~cluster, type = "scatter3d") 
# Show the plot
plot_3d


```

# Exploratory data analysis

## Grids of box plots for each parameters

```{r}
pca_eda <- tibble(df_pre_pca, cluster = as.factor(kmeans_result$cluster))

library(ggplot2)
library(gridExtra)

# Convert the data frame from a wide to a long format  
pca_eda_long <- pca_eda|> 
  tidyr::gather(key = "variable", value = "value", -cluster)

#Create a list of ggplot objects for each variable
plots <- lapply(unique(pca_eda_long$variable), function(var) {
  ggplot(pca_eda_long[pca_eda_long$variable == var,], aes(x = cluster, y = value)) +
    geom_boxplot() +
    labs(title = var, x = "Cluster", y = "Value") +
    theme_minimal()
})
grid_plot <- do.call(gridExtra::grid.arrange, c(plots, ncol = 5))

ggsave("output/boxplots.png", grid_plot, width = 20, height = 15)

```

## Visualise the clusters into a map

# Read a map then attach

```{r}
library(terra)
library(sf)
# Read the shapefile
desa <- st_read("data/INDO_DESA_2019/INDO_DESA_2019.shp")

# Filter based on the 'kdprov' attribute
desa_sulsel <- desa |> filter(kdprov %in% 73)
desa_sulsel <- desa_sulsel |> dplyr::select(-c("kddesa", "iddesa"))
desa_sulsel_id_kec <- desa_sulsel |> select(nmkab,nmkec,kdprov,kdkab,kdkec) |> 
  mutate(idkec_dum = paste0(kdprov,kdkab,kdkec, "a") )
rm(desa)

# add the cluster attribute to the desa_sulsel object
cluster_data<- scaled_df_complete_temp |> select(-c(nmkab, nmkec ,nmprov ,  sumber, kdprov,kdkab,kdkec)) |> 
  dplyr::select(1:9) |> bind_cols(tibble (cluster = kmeans_result$cluster))

clusters_sulsel<- desa_sulsel_id_kec |> left_join(cluster_data, by = "idkec_dum")

# Write the sf object to a shapefile
st_write(clusters_sulsel, "output/tipologi_5_kelas.shp", append = TRUE)

# # Plot the SpatVector object, using the 'cluster' attribute for the fill color
# plot(desa_sulsel, "cluster", col=c("#E69F00", "#CC79A7", "#009E73", "#F0E442", "#0072B2"), lwd=0.1) # Assuming we have 5 clusters

library(leaflet)
# Constructing the label string first
clusters_sulsel$label_content <- with(clusters_sulsel, 
                                      paste0("<strong>Kabupaten:</strong> ", nmkab, "<br>",
                                             "<strong>Kecamatan:</strong> ", nmkec, "<br>",
                                             "<strong>Cluster:</strong> ", cluster)) |> lapply(htmltools::HTML)

# Check the first few rows to ensure the label is being constructed correctly
head(clusters_sulsel$label_content)

# Create the leaflet map with HTML-rendered labels
leaflet(clusters_sulsel) %>%
  addProviderTiles(providers$OpenStreetMap) %>%
  addPolygons(
    fillColor = ~pal(cluster),
    weight = 0.5,
    opacity = 1,
    color = "white",
    dashArray = "3",
    fillOpacity = 0.7,
    highlight = highlightOptions(
      weight = 5,
      color = "#666",
      dashArray = "",
      fillOpacity = 0.7,
      bringToFront = TRUE
    ),
    label = ~label_content,
    labelOptions = labelOptions(
      noHide = FALSE,
      direction = 'auto'
    )
  ) %>%
  addLegend(pal = pal, values = ~cluster, title = "Cluster")


```

# Caveats

-   **Lack of Data Understanding**: The analysis was conducted without in-depth knowledge of the data, so the results should be interpreted cautiously.

-   **Skewed Distribution**: Many variables have a right-skewed distribution, which may violate the linearity assumption if not properly transformed.

-   **No one-size-fits-all**: Data pre-processing should be tailored to the specific dataset and analysis objectives. This includes careful consideration of data transformation and variable selection, based on both statistical properties and subject-matter knowledge.
